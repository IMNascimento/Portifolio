<!DOCTYPE html>
<html lang="pt-br">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Camada de Atenção em Transformers: Teoria, Prática e Código</title>
  <link rel="stylesheet" href="./../assets/pages.css">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet">
  <!-- MathJax para fórmulas LaTeX -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']], displayMath: [['$$', '$$'], ['\\[', '\\]']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <a href="javascript:history.back()" class="btn-voltar" title="Voltar">
    <svg width="25" height="25" viewBox="0 0 20 20" fill="none">
      <circle cx="10" cy="10" r="9" stroke="#00ffd0" stroke-width="2"/>
      <polyline points="11.5,6 8,10 11.5,14" fill="none" stroke="#00ffd0" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
    </svg>
  </a>

  <header>
    <span class="main-illustration">
      <!-- Ícone de olho estilizado (atenção) -->
      <svg class="svg-center" width="72" height="72" viewBox="0 0 72 72" fill="none">
        <ellipse cx="36" cy="36" rx="34" ry="32" fill="#f2fcff" stroke="#00bcd4" stroke-width="3"/>
        <ellipse cx="36" cy="36" rx="14" ry="12" fill="#b2ebf2" stroke="#00bcd4" stroke-width="2"/>
        <ellipse cx="36" cy="36" rx="6" ry="5" fill="#00bcd4"/>
      </svg>
    </span>
    <h1>Camada de Atenção em Transformers: Teoria, Prática e Código</h1>
    <div class="subtitle">
      Entenda na prática como a atenção revolucionou o processamento de linguagem.
    </div>
  </header>

  <main>
    <section>
      <h2>O que é e por que atenção é tão importante?</h2>
      <p>
        A <b>camada de atenção</b> é o coração dos modelos Transformer. Ela permite que, ao processar cada token, o modelo "olhe" para todos os outros tokens da sequência e pese o quanto cada um deles é relevante para gerar a próxima saída.
      </p>
      <ul>
        <li>Capta <b>relações de longo alcance</b> em frases e textos inteiros.</li>
        <li>Permite <b>processamento paralelo</b>, acelerando treinamento.</li>
        <li>É a base para os avanços de LLMs como GPT, Llama e Gemini.</li>
      </ul>
      <p>
        Modelos anteriores (RNNs/LSTMs) só enxergavam o passado imediato. A atenção revolucionou o campo ao permitir visão global sobre a sequência.
      </p>
    </section>

    <section>
      <h2>Como a atenção funciona?</h2>
      <p>
        Cada palavra/token vira três vetores:
        <ul>
          <li><strong>Query (Q):</strong> O que eu quero buscar?</li>
          <li><strong>Key (K):</strong> O que cada token representa?</li>
          <li><strong>Value (V):</strong> Qual informação carregar?</li>
        </ul>
        Para cada par (Q, K), calcula-se uma "pontuação" de atenção, define-se o peso de cada token, e faz-se uma soma ponderada dos Values. O resultado é um novo vetor contextualizado para cada token.
      </p>
      <div style="overflow-x: auto;">
      <pre style="background:#21262c;color:#fff;padding:1em;border-radius:8px;">
# Atenção escalada simples em NumPy
import numpy as np

def softmax(x):
    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return e_x / e_x.sum(axis=-1, keepdims=True)

# Suponha Q, K, V como matrizes (batch_size, seq_len, d_k)
def attention(Q, K, V):
    d_k = Q.shape[-1]
    scores = np.matmul(Q, K.transpose(0,2,1)) / np.sqrt(d_k)
    weights = softmax(scores)
    return np.matmul(weights, V)  # (batch_size, seq_len, d_v)
      </pre>
      </div>
      <p>
        <i>O segredo:</i> Tudo pode ser calculado em lote (batch), e a fórmula se aplica para textos, imagens, áudios e mais.
      </p>
      <p style="text-align:center">
        <b>Fórmula matemática:</b><br>
        \[
          \mathrm{Attention}(Q, K, V) = \mathrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
        \]
      </p>
    </section>

    <section>
      <h2>Visualizando a Atenção</h2>
      <svg class="svg-center" xmlns="http://www.w3.org/2000/svg" width="520" height="220" viewBox="0 0 520 220">
        <rect width="520" height="220" rx="16" fill="#f2fcff"/>
        <!-- Tokens de entrada -->
        <g>
          <rect x="50" y="45" width="90" height="36" rx="8" fill="#fff" stroke="#00bcd4" stroke-width="2"/>
          <text x="95" y="68" text-anchor="middle" font-family="monospace" font-size="18" fill="#00bcd4">Eu</text>
          <rect x="160" y="45" width="90" height="36" rx="8" fill="#fff" stroke="#00bcd4" stroke-width="2"/>
          <text x="205" y="68" text-anchor="middle" font-family="monospace" font-size="18" fill="#00bcd4">gosto</text>
          <rect x="270" y="45" width="90" height="36" rx="8" fill="#fff" stroke="#00bcd4" stroke-width="2"/>
          <text x="315" y="68" text-anchor="middle" font-family="monospace" font-size="18" fill="#00bcd4">de</text>
          <rect x="380" y="45" width="90" height="36" rx="8" fill="#fff" stroke="#00bcd4" stroke-width="2"/>
          <text x="425" y="68" text-anchor="middle" font-family="monospace" font-size="18" fill="#00bcd4">pizza</text>
        </g>
        <!-- Setas de atenção -->
        <g stroke="#b2ebf2" stroke-width="3" opacity="0.7">
          <line x1="95" y1="81" x2="95" y2="170"/>
          <line x1="95" y1="81" x2="205" y2="170"/>
          <line x1="95" y1="81" x2="315" y2="170"/>
          <line x1="95" y1="81" x2="425" y2="170"/>
        </g>
        <!-- Token de saída -->
        <rect x="190" y="160" width="140" height="40" rx="10" fill="#fffde7" stroke="#ffb300" stroke-width="2"/>
        <text x="260" y="185" text-anchor="middle" font-family="monospace" font-size="20" fill="#ffb300">"Eu"</text>
        <text x="260" y="210" text-anchor="middle" font-family="Inter, sans-serif" font-size="12" fill="#555">Token em foco recebe contexto de todos</text>
      </svg>
      <p style="text-align:center;font-size:0.95em;color:#aaa">
        Atenção: cada token considera todos os outros ao gerar sua saída.
      </p>
    </section>

    <section>
      <h2>Exemplo prático em PyTorch</h2>
      <p>
        Veja como calcular self-attention com apenas algumas linhas usando PyTorch. Neste exemplo, temos uma sequência simples com 3 tokens e dimensão 4.
      </p>
      <div style="overflow-x: auto;">
      <pre style="background:#21262c;color:#fff;padding:1em;border-radius:8px;">
import torch
import torch.nn.functional as F

Q = torch.rand(1, 3, 4)  # (batch, seq_len, d_k)
K = torch.rand(1, 3, 4)
V = torch.rand(1, 3, 4)
d_k = Q.size(-1)

# Scaled dot-product attention
scores = torch.matmul(Q, K.transpose(-2, -1)) / d_k ** 0.5  # (1, 3, 3)
weights = F.softmax(scores, dim=-1)
output = torch.matmul(weights, V)  # (1, 3, 4)

print("Pesos de atenção:", weights)
print("Saída contextualizada:", output)
      </pre>
      </div>
      <p>
        Assim, você pode visualizar os pesos de atenção que cada token atribui a todos os outros — e entender, por exemplo, que “pizza” pode depender muito de “gosto” e menos de “eu”.
      </p>
    </section>

    <section>
      <h2>Multi-Head Attention: mais contexto, mais poder</h2>
      <p>
        Na prática, os Transformers usam diversas "cabeças" de atenção em paralelo. Cada uma aprende relações diferentes e, depois, seus resultados são combinados. Isso permite capturar <b>diversos padrões simultaneamente</b>.
      </p>
      <div style="overflow-x: auto;">
      <pre style="background:#21262c;color:#fff;padding:1em;border-radius:8px;">
import torch
import torch.nn as nn

multi_head = nn.MultiheadAttention(embed_dim=8, num_heads=2, batch_first=True)
x = torch.rand(1, 5, 8)  # (batch, seq_len, embedding_dim)
attn_output, attn_weights = multi_head(x, x, x)
print("Saída com Multi-Head:", attn_output.shape)
print("Pesos das heads:", attn_weights.shape)
      </pre>
      </div>
      <p>
        Usando <code>nn.MultiheadAttention</code> você ganha performance e flexibilidade, sem precisar implementar tudo na mão.
      </p>
    </section>

    <section>
      <h2>Onde a atenção é aplicada?</h2>
      <ul>
        <li>Tradução automática, resumo de textos, chatbots e LLMs</li>
        <li>Análise de imagens (<a href="https://arxiv.org/abs/2010.11929" target="_blank">Vision Transformers</a>)</li>
        <li>Reconhecimento de fala e áudio</li>
        <li>Até jogos e recomendação!</li>
      </ul>
      <p>
        Hoje, o conceito de atenção já foi além do texto e se tornou onipresente em IA.
      </p>
    </section>

    <section>
      <h2>Resumo e referências</h2>
      <p>
        A camada de atenção é uma engrenagem fundamental para IA moderna — e dominar sua teoria, exemplos e prática te coloca na frente. Se quiser se aprofundar:
      </p>
      <ul>
        <li>
          <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank">
            The Illustrated Transformer (Jay Alammar)
          </a>
        </li>
        <li>
          <a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html" target="_blank">
            Documentação PyTorch MultiheadAttention
          </a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/1706.03762" target="_blank">
            Paper original: "Attention is All You Need"
          </a>
        </li>
      </ul>
      <p>
        Nos próximos artigos, vamos explorar:
        <ul>
          <li>Implementação de Attention em NLP</li>
          <li>Visualização de heads e interpretabilidade</li>
          <li>Aplicações práticas em datasets reais</li>
        </ul>
        Fique de olho!
      </p>
    </section>
  </main>

  <footer>
    Desenvolvido por 
    <a href="https://www.linkedin.com/in/igor-m-nascimento/" target="_blank">Igor Nascimento</a> 
    – <span id="ano-atual"></span> | &copy; <a href="https://sophialabs.com.br" target="_blank">SophiaLabs</a>
  </footer>

  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const yearSpan = document.getElementById('ano-atual');
      if (yearSpan) yearSpan.textContent = new Date().getFullYear();
    });
  </script>
</body>
</html>
